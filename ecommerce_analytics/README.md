# üìò Documenta√ß√£o do Projeto: E-commerce Analytics com dbt e Airflow

## üéØ Vis√£o Geral do Projeto

Este projeto tem como objetivo principal o desenvolvimento e implementa√ß√£o de um pipeline de engenharia de dados anal√≠tico moderno, focado em **an√°lise de vendas** e **previs√£o de churn** para um e-commerce simulado. A solu√ß√£o integra ferramentas como **dbt** para modelagem de dados e **Apache Airflow (via Astro CLI)** para orquestra√ß√£o, culminando na gera√ß√£o de **relat√≥rios semanais automatizados**.

-----

## üß± Estrutura do Reposit√≥rio

O reposit√≥rio do projeto est√° organizado para otimizar a clareza e a manuten√ß√£o dos componentes:

```
ecommerce_analytics/
‚îú‚îÄ‚îÄ airflow/            # Cont√©m as defini√ß√µes de DAGs e operadores customizados do Airflow.
‚îú‚îÄ‚îÄ dbt/                # Abriga o projeto dbt, incluindo modelos de dados, testes e seeds.
‚îú‚îÄ‚îÄ scripts/            # Scripts utilit√°rios para gera√ß√£o de dados fict√≠cios.
‚îú‚îÄ‚îÄ reports/            # Templates para a gera√ß√£o dos relat√≥rios HTML.
‚îú‚îÄ‚îÄ pyproject.toml      # Lista de depend√™ncias Python do projeto.
‚îî‚îÄ‚îÄ README.md           # Documenta√ß√£o principal do projeto (este arquivo).
```

-----

## ‚úÖ Etapas de Implementa√ß√£o

As seguintes etapas detalham o processo de configura√ß√£o e desenvolvimento do pipeline:

### 1\. Configura√ß√£o do Ambiente

Para iniciar o desenvolvimento, √© essencial preparar o ambiente com as ferramentas necess√°rias:

  * **Instala√ß√£o do Astro CLI:**
    Siga as instru√ß√µes oficiais para instalar o Astro CLI, que facilitar√° a orquestra√ß√£o local do Airflow:
    ```bash
    https://www.astronomer.io/docs/astro/cli/install-cli
    ```
  * **Inicializa√ß√£o do Projeto Astro:**
    Crie a estrutura b√°sica do projeto Airflow utilizando o Astro CLI:
    ```bash
    astro dev init
    ```
  * **Configura√ß√£o do Ambiente Virtual/Container:**
    Crie e ative um ambiente virtual (ou utilize um container Docker) e instale as depend√™ncias Python listadas em `pyproject.toml`:
      * `dbt-core` e `dbt-duckdb` (Optamos por usar um banco de dados local)
      * `apache-airflow`
      * `faker` (para gera√ß√£o de dados)
      * `jinja2` (para templating de relat√≥rios)
      * `pandas` (para manipula√ß√£o de dados)

### 2\. Gera√ß√£o de Dados Fict√≠cios

O projeto utiliza dados simulados para replicar um cen√°rio de e-commerce:

  * **Script:** `scripts/generate_fake_data.py`
  * **Funcionalidade:** Este script √© respons√°vel por gerar os seguintes conjuntos de dados fict√≠cios em formato CSV:
      * `customers.csv`
      * `orders.csv`
      * `products.csv`
      * `order_items.csv`
      * `cancelamentos.csv`
  * **Armazenamento:** Os arquivos CSV gerados devem ser armazenados no diret√≥rio `dbt/seeds/`, permitindo que sejam carregados no Data Warehouse via `dbt seed`.

### 3\. Configura√ß√£o do Projeto dbt

O projeto dbt (`dbt/`) √© a camada de transforma√ß√£o e modelagem de dados:

  * **Inicializa√ß√£o:**
    Inicie o projeto dbt no diret√≥rio `dbt/`:

    ```bash
    dbt init ecommerce_analytics
    ```
  * **Cria√ß√£o do profile.yml**
  Crie dentro do arquivo de instala√ß√£o do .dbt o profiles.yml da seguinte maneira:

    ```bash
    dbt_ecommerce:
    target: dev
    outputs:
      dev:
        type: duckdb
        path: data/dbt_ecommerce.duckdb  # banco local em arquivo .duckdb
        schema: staging
    ```
  

  * **Carregamento de Seeds:**
    Ap√≥s colocar os arquivos CSV gerados em `dbt/seeds/`, execute o comando para carregar os dados no seu Data Warehouse:

    ```bash
    dbt seed
    ```
  

  * **Modelagem de Dados (Camadas):**
    Os modelos dbt s√£o organizados em tr√™s camadas distintas para garantir modularidade e clareza:

      * **Staging (`models/staging/`):** Modelos brutos que padronizam e limpam os dados das fontes.
          * `stg_customers.sql`
          * `stg_orders.sql`
          * `stg_products.sql`
      * **Intermediate (`models/intermediate/`):** Modelos que realizam transforma√ß√µes intermedi√°rias complexas, servindo de base para os marts.
          * `int_churn_prediction_data.sql`
      * **Marts (`models/marts/`):** Modelos agregados e prontos para consumo por ferramentas de BI e relat√≥rios.
          * `fct_sales.sql` (Fato de vendas)
          * `dim_customer.sql` (Dimens√£o de clientes)
          * `dim_product.sql` (Dimens√£o de produtos)

  * **Testes de Qualidade de Dados:**
    Implemente testes de dbt (e.g., `not_null`, `unique`) nos modelos para garantir a integridade e a qualidade dos dados. Execute os testes com:

  Para isto, dentro de cada pasta das camadas crie o arquivo schema.yml com os testes.
  
  Ap√≥s criado, rode:

  ```bash
  dbt test
  ```
    
### 4\.Visualiza√ß√£o na interface gr√°fica:

Voc√™ consegue visualizar as informa√ß√µes e dados das tabelas, usando o comando:
```
dbt docs generate
dbt docs serve
```
Estes comandos v√£o gerar documenta√ß√£o e voc√™ poder√° ver a estrutura dos dados.

### 5\. Cria√ß√£o da DAG no Airflow

A orquestra√ß√£o do pipeline √© realizada atrav√©s de uma Directed Acyclic Graph (DAG) no Airflow:

  * **Localiza√ß√£o:** `airflow/dags/ecommerce_dag.py`
  * **Tarefas (Tasks):** A DAG deve incluir as seguintes tarefas, executadas em sequ√™ncia l√≥gica:
      * `generate_data`: Invoca o script de gera√ß√£o de dados fict√≠cios.
      * `load_to_warehouse`: (Opcional, se a carga inicial n√£o for feita pelo `dbt seed` ou requerer um passo ETL separado).
      * `dbt_seed`: Executa o comando `dbt seed` para carregar dados iniciais.
      * `dbt_run`: Executa os modelos dbt para transformar os dados.
      * `dbt_test`: Roda os testes de qualidade dos dados ap√≥s a transforma√ß√£o.
      * `send_report`: Gera o relat√≥rio semanal e o envia (e.g., por e-mail).
  * **Agendamento:** A DAG deve ser agendada para execu√ß√£o **semanal** (`@weekly`).

### 6\. Relat√≥rio HTML Semanal

Um relat√≥rio consolidado √© gerado semanalmente para an√°lise das m√©tricas de neg√≥cio:

  * **Localiza√ß√£o:** `reports/weekly_report.html` (template)
  * **Tecnologia:** Utiliza√ß√£o do Jinja2 para templating din√¢mico, permitindo a inser√ß√£o de dados calculados pelos modelos dbt.
  * **Conte√∫do:** O relat√≥rio deve incluir as seguintes m√©tricas e informa√ß√µes:
      * Total de vendas do per√≠odo.
      * Produtos com maior margem de lucro.
      * Taxa de churn de clientes.
      * Ticket m√©dio por categoria de produto.

### 7\. Execu√ß√£o e Valida√ß√£o

Ap√≥s a implementa√ß√£o das etapas anteriores, √© crucial validar o funcionamento do pipeline:

  * **Execu√ß√£o Local:** Execute o pipeline completo utilizando o Astro CLI no ambiente local para simular a produ√ß√£o.
  * **Valida√ß√£o dbt:** Verifique os resultados dos modelos dbt no seu Data Warehouse para garantir que os dados est√£o corretos e as transforma√ß√µes foram aplicadas adequadamente.
  * **Teste de Envio de Relat√≥rio:** Confirme se o relat√≥rio HTML √© gerado corretamente e se o mecanismo de envio (e.g., e-mail) funciona conforme o esperado.

-----

## üìå KPIs do Projeto

As m√©tricas chave de desempenho (KPIs) monitoradas por este projeto, juntamente com suas respectivas fontes no projeto dbt, s√£o as seguintes:

| M√©trica                   | Fonte dbt                 | Descri√ß√£o                                                                       |
| :------------------------ | :------------------------ | :------------------------------------------------------------------------------ |
| Receita total             | `fct_sales`               | Soma do valor total das vendas realizadas no per√≠odo.                           |
| Ticket m√©dio por categoria | `fct_sales` + `dim_product` | Valor m√©dio das vendas, segmentado pelas categorias de produtos.                |
| Produtos com maior margem | `dim_product`             | Identifica os produtos que geram maior lucro bruto para o e-commerce.           |
| Taxa de churn             | `dim_customer`            | Percentual de clientes que cancelaram suas contas ou deixaram de comprar.       |
| Clientes fi√©is            | `int_churn_prediction_data` | Identifica clientes com alta frequ√™ncia de compra ou longo hist√≥rico no e-commerce. |

-----

## üéÅ Funcionalidades Adicionais (Opcionais)

Para aprimorar ainda mais o projeto, as seguintes funcionalidades podem ser exploradas:

  * **Painel de BI:** Integra√ß√£o com ferramentas de Business Intelligence como Metabase, Looker Studio ou dashboards interativos constru√≠dos com Streamlit para visualiza√ß√£o dos KPIs.
  * **Alertas Automatizados:** Configura√ß√£o de alertas por e-mail (ou outras plataformas de notifica√ß√£o) quando a taxa de churn exceder um determinado limite.
  * **Dashboards Temporais:** Implementa√ß√£o de dashboards que mostrem a evolu√ß√£o dos KPIs ao longo do tempo (e.g., √∫ltimos 30 dias, 6 meses), permitindo an√°lises de tend√™ncia.

-----